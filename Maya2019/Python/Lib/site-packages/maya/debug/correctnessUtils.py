"""
This file holds utilities used to write correctness tests.

It is meant to be used by tests validating the correctness of evaluation, for
instance validating the correctness of evaluation under evaluation manager or
background evaluation.

Its main utility function is run_correctness_test().

If results_path is set then the graph output and differences are dumped to
files using that path as the base name.  For example:

    results_path            = MyDirectory/emCorrecteness_animCone
    reference results dump  = MyDirectory/emCorrecteness_animCone.ref.txt
    reference results image = MyDirectory/emCorrecteness_animCone.ref.png
    ${mode} results dump    = MyDirectory/emCorrecteness_animCone.${mode}.txt
    ${mode} results image   = MyDirectory/emCorrecteness_animCone.${mode}.png

If results_path is not set then no output is stored, everything is live.

The return value is a list of value pairs indicating number of differences
between the reference evaluation and the tested mode. e.g. if you requested 'ems'
mode then you would get back {'ems' : (0, 0, 0)} from a successful comparison.

If file_name is not set then the current scene is analyzed.
"""

import json
import os
import os.path
import tempfile
import maya.cmds as cmds
from maya.debug.TODO import TODO
from maya.debug.DGState import DGState
from maya.debug.emModeManager import emModeManager
from maya.debug.PlaybackManager import PlaybackManager


__all__ = [ 'run_correctness_test'
        ,   'multichain_nodes'
        ,   'CORRECTNESS_MAX_FRAMECOUNT'
        ,   'CORRECTNESS_NO_SETUP'
        ,   'CORRECTNESS_DOUBLE_PLAYBACK'
        ,   'CORRECTNESS_INVALIDATE'
        ,   'CORRECTNESS_LOAD'
        ]

# Maximum number of frames to play (to avoid eternal tests)
CORRECTNESS_MAX_FRAMECOUNT = 20

# Setup modes when analyzing scenes in EM modes
CORRECTNESS_NO_SETUP        = 0 # Do nothing, just run playback
CORRECTNESS_DOUBLE_PLAYBACK = 1 # Run playback twice to ensure graph is valid
CORRECTNESS_INVALIDATE      = 2 # Invalidate the graph to force rebuild
CORRECTNESS_LOAD            = 4 # Load the file between every mode (if the file was loaded at all)

# Name of the cache evaluator
CACHE_EVALUATOR = 'cache'

#======================================================================

def model_panel_visible():
    '''
    Return true if any model panel is currently visible. This includes
    checking for GUI model and looking at the currently visible panels
    to see if any of them are model panels.
    '''
    if cmds.about(batch=True):
        return False

    panel_list = cmds.getPanel( visiblePanels=True ) or []
    for visible_panel in panel_list:
        try:
            cmds.modelPanel( visible_panel, query=True, modelEditor=True )
            return True
        except RuntimeError:
            pass
    return False

#======================================================================

class EmptyContext(object):
    """
    Empty context class that performs no action on entry or exit.
    """
    @staticmethod
    def should_pull_values():
        ''':return: True, empty contexts always pull'''
        return True

    def __enter__(self):
        '''Simple entry of empty context'''
        return self

    @staticmethod
    def __exit__(exit_type,value,traceback):
        '''Simple exit of empty context'''
        pass

    def __str__(self):
        '''Empty context has no parameters'''
        return '{ "Empty Context" : {} }'

#======================================================================

def __is_maya_file(path):
    """
    Check to see if the given path is a Maya file. Only looks for native Maya
    files ".ma" and ".mb", not other importable formats such as ".obj" or ".dxf"
    """
    return os.path.isfile(path) and ((path[-3:] == '.ma') or (path[-3:] == '.mb'))


#======================================================================

def __find_em_plugs(relevant_nodes, results_filename):
    """
    Find all of the root level plugs that the EM will be marking
    dirty. The passed-in dictionary will be populated by a list of
    dictionaries.

    :param relevant_nodes:  Array of [NODES_TO_INCLUDE]

    :return: Dictionary of NODE -> [DIRTY_PLUG_IN_NODE]
    """
    em_plugs = {}
    plug_file = results_filename
    try:
        # If a specific location for the plug file wasn't set then use a temp file.
        # A file has to be used because the output can grow pretty large.
        if results_filename is None:
            plug_file = os.path.join( tempfile.gettempdir(), 'EMPlugs.txt' )
        cmds.dbpeek(op='graph', eg=True, all=True, a='plugs', of=plug_file)
        json_plugs = json.load(open(plug_file, 'r'))
    except ValueError:
        print 'WARNING: No output from plug list'
        return em_plugs

    if not json_plugs or 'plugs' not in json_plugs:
        print 'WARNING: No output from plug list'
        return em_plugs

    for node, per_node_list in json_plugs['plugs'].iteritems():
        if node not in relevant_nodes:
            continue
        input_plugs = per_node_list['input']
        output_plugs = per_node_list['output']
        world_plugs = per_node_list['affectsWorld']
        attribute_plugs = per_node_list['attributes']
        em_plugs[node] = list(set(input_plugs + output_plugs + world_plugs + attribute_plugs))

    # If a file name was passed in then write out the filtered plug list
    try:
        if results_filename is None:
            json.dump( { 'filtered plugs' : em_plugs }, open(plug_file, 'w') )
    except Exception, ex:
        print 'WARNING: Filtered list was not output : {}'.format(ex)

    return em_plugs

#======================================================================

def multichain_nodes():
    '''
    The IK multi-chain solver is known to create inconsistent results so remove
    any joints that are being controlled by it from the list being compared.
    Even though we have an evaluator that disables the EM when these are found even
    the DG results are not consistent so these always must be ignored.
    :return: Set of nodes that are affected by the multi-chain solver
    '''
    node_set = set()
    for node in cmds.ls(type='ikHandle'):
        try:
            solver_type = None
            solver_type = cmds.nodeType(cmds.ikHandle(node, query=True, solver=True))
        except Exception:
            pass

        # Any other kind of IK solver is fine
        if solver_type != 'ikMCsolver':
            continue

        multi_chain_joints = cmds.ikHandle(node, query=True, jointList=True)
        if multi_chain_joints is not None:
            node_set.add( set(multi_chain_joints) )

        multi_chain_effector = cmds.ikHandle(node, query=True, endEffector=True)
        if multi_chain_effector is not None:
            node_set.add( multi_chain_effector )

    return node_set

#======================================================================

def run_correctness_test( reference_mode
                        , modes
                        , file_name
                        , results_path
                        , verbose
                        , max_frames
                        , data_types
                        , em_setup ):
    """
    Evaluate the file in multiple modes and compare the results.

    Modes are objects that must contain at least the following methods:
        title   : returns a string describing the mode
        em_mode : returns a string to be passed to emModeManager.setMode()
                  before running the test.
        context : returns a context object that can set extra state on enter
                  and reset it on exit (or None if not needed).
        relevant_nodes : takes a set of potential nodes to use and filters them,
                         returning the subset of those nodes to which the mode is applicable

    :param reference_mode: Mode against which the test runs will be compared
    :param modes:          List of modes to run the tests in.
    :param file_name:      Name of file to load for comparison. None means use the current scene
    :param results_path:   Where to store the results. None means don't store anything
    :param verbose:        If True then dump the differing values when they are encountered
    :param max_frames:     Maximum number of frames in the playback, to avoid long tests.
    :param data_types:     List of data types to include in the analysis. These are the possibilities:
                               matrix: Any attribute that returns a matrix
                               vector: Any attribute with type 3Double
                               vertex: Attributes on the mesh shape that hold vertex positions
                               number: Any attribute that returns a number
                               screen: Screenshot after the animation runs
    :param em_setup:       What to do before running an EM mode test, in bitfield combinations
                               CORRECTNESS_NO_SETUP        Do nothing, just run playback
                               CORRECTNESS_DOUBLE_PLAYBACK Run playback twice to ensure graph is valid
                               CORRECTNESS_INVALIDATE      Invalidate the graph to force rebuild
                               CORRECTNESS_LOAD            Load the file between every mode's run
                                                           (Default is to just load once at the beginning.)

    :return: a list of value tuples indicating the run mode and the number of
             changes encountered in that mode. e.g. ['ems', 0]

    If verbose is true then instead of counts return a list of actual changes.
    e.g. ['ems', ["plug1,oldValue,newValue"]]

    Changed values are a CSV 3-tuple with "plug name", "value in reference mode", "value in the named test mode"
    in most cases.

    In the special case of an image difference the plug name will be one
    of the special ones below and the values will be those generated by the
    comparison method used:
        DGState.SCREENSHOT_PLUG_MD5 : md5 values when the image compare could not be done
        DGState.SCREENSHOT_PLUG_MAG : md5 and image difference values from ImageMagick
        DGState.SCREENSHOT_PLUG_IMF : md5 and image difference values from imf_diff
    """
    # Fail if the file_name is not a valid Maya file.
    if file_name != None and not __is_maya_file(file_name):
        print 'ERROR: %s is not a Maya file' % file_name
        return {}

    # Turn off cycle check during this test, the output is useless and slows the test down
    old_cycle_check = cmds.cycleCheck( query=True, evaluation=True )
    cmds.cycleCheck( evaluation=False )

    comparisons = {}
    try:
        # Load the file_name if it was specified, otherwise the current scene will be used
        if file_name != None:
            cmds.file(file_name, force=True, open=True)

        ref_results = None
        ref_results_image = None

        # Using lists allows me to do a comparison of two identical modes.
        # If results_path is given then the second and successive uses of the
        # same type will go into files with an incrementing suffix (X.ref.txt,
        # X.ref1.txt, X.ref2.txt...)
        mode_results_files = []
        mode_compare_files = []
        mode_results_image_files = []
        results = []
        em_plug_file_name = None

        # Create a list of unique mode suffixes, appending a count number whenever
        # the same mode appears more than once on the modes list.
        mode_counts = {}
        unique_modes = []
        mode_counts['ref'] = 1
        for mode_object in modes:
            mode = mode_object.title()
            mode_counts[mode] = mode_counts.get(mode, 0) + 1
            suffix = ''
            if mode_counts[mode] > 1:
                suffix = str(mode_counts[mode])
            unique_modes.append('%s%s' % (mode, suffix))

        if results_path != None:
            # Make sure the path exists
            if not os.path.isdir(results_path):
                os.makedirs(results_path)

            em_plug_file_name = os.path.join(results_path, 'EMPlugs.txt')

            # Build the rest of the paths to the results files.
            # If no file was given default the results file prefix to "SCENE".
            if file_name != None:
                # Absolute paths cannot be appended to the results path. Assume
                # that in those cases just using the base name is sufficient.
                if os.path.isabs(file_name):
                    results_path = os.path.join(results_path, os.path.basename(file_name))
                else:
                    results_path = os.path.join(results_path, file_name)
            else:
                results_path = os.path.join(results_path, 'SCENE')

            ref_results = '%s.ref.txt' % results_path
            mode_counts['ref'] = 1

            for mode in unique_modes:
                # mode strings can have '/' which are illegal in file_name, replace with '='.
                mode = mode.replace('/', '=')
                mode_results_files.append('%s.%s.txt' % (results_path, mode))
                mode_compare_files.append('%s.DIFF.%s.txt' % (results_path, mode))
        else:
            # Still need the file args to pass in to DGState. None = don't output.
            for _ in modes:
                mode_results_files.append( None )
                mode_compare_files.append( None )

        # If the image comparison was requested figure out where to store the
        # file. Done separately because even if the files won't be saved the image
        # comparison needs to dump a file out for comparison.
        if 'screen' in data_types:
            if results_path == None:
                image_dir = tempfile.gettempdir()
                if file_name != None:
                    # Absolute paths cannot be appended to the results path. Assume
                    # that in those cases just using the base name is sufficient.
                    if os.path.isabs(file_name):
                        image_dir = os.path.join(image_dir, os.path.basename(file_name))
                    else:
                        image_dir = os.path.join(image_dir, file_name)
                else:
                    image_dir = os.path.join(image_dir, 'SCENE')
            else:
                image_dir = results_path

            ref_results_image = '%s.ref.png' % image_dir
            for mode in unique_modes:
                # mode strings can have '/' which are illegal in file_name, replace with '='.
                mode = mode.replace('/', '=')
                mode_results_image_files.append('%s.%s.png' % (image_dir, mode))
        else:
            ref_results_image = None
            for _ in unique_modes:
                mode_results_image_files.append( None )

        em_plugs = None

        comparison_mode = DGState.OUTPUT_JSON if verbose else DGState.OUTPUT_CSV

        # If no model panel is visible the refresh command won't trigger any evaluation
        # so we have to force everything to be dirty
        if not model_panel_visible():
            cmds.dgdirty(allPlugs=True)

        # Record the reference evaluation version of the results
        with emModeManager() as em_mgr, PlaybackManager() as play_mgr:

            # Set to free running but hit every frame
            play_mgr.framesPerSecond  = 0.0
            play_mgr.maxPlaybackSpeed = 0.0
            play_mgr.set_limited_range( max_frames=max_frames, from_start=True )

            # Build the evaluation graph in an initial pass. Mode doesn't matter since the graph
            # will be the same for all EM modes.
            em_mgr.setMode( 'emp' )
            em_mgr.rebuild( False )
            try:
                # Only the nodes relevant to the reference mode need be considered
                potential_nodes = set( json.loads( cmds.dbpeek( op='graph', eg=True, a='nodes', all=True ) )['nodes'] )
            except Exception:
                potential_nodes = set()

            # Start with the EM in the reference mode
            reference_context = reference_mode.context()

            with reference_context:
                em_mgr.setMode(reference_mode.em_mode())

                if (em_setup & CORRECTNESS_DOUBLE_PLAYBACK) != 0:
                    play_mgr.play_all()
                play_mgr.play_all()

                reference_state = DGState()

                # Catch the case when the EM has been disabled due to unsupported areas in the graph,
                # or the reference mode was just plain DG evaluation.
                # When that happens the evaluation has to be forced or the values will be wrong.
                reference_state.scan_scene(do_eval=reference_context.should_pull_values(), data_types=data_types)
                reference_state.store_state(ref_results, ref_results_image)

            # Walk all of the modes requested and run the tests for them
            for mode_num in range(len(modes)):
                test_mode = modes[mode_num]
                em_mgr.setMode( test_mode.em_mode() )
                extra_context = test_mode.context()

                if not extra_context:
                    extra_context = EmptyContext()

                with extra_context:

                    if (em_setup & CORRECTNESS_LOAD != 0) and file_name != None:
                        cmds.file(file_name, force=True, open=True)

                    if (em_setup & CORRECTNESS_INVALIDATE) != 0:
                        em_mgr.rebuild( include_scheduling=True )

                    if (em_setup & CORRECTNESS_DOUBLE_PLAYBACK) != 0:
                        play_mgr.play_all()

                    play_mgr.play_all()

                    # Each mode will have a different set of caching and invisibilty nodes so this
                    # has to be redone each time.
                    per_context_nodes = reference_mode.relevant_nodes( potential_nodes )

                    # The EM plugs are the same for all partitionings so remember them after
                    # they were collected for the first time.
                    if em_plugs == None:
                        em_plugs = __find_em_plugs(per_context_nodes, em_plug_file_name)
                        reference_state.filter_state(em_plugs)

                    mode_state = DGState()
                    mode_state.scan_scene(do_eval=extra_context.should_pull_values(), data_types=data_types)
                    mode_state.store_state(mode_results_files[mode_num], mode_results_image_files[mode_num])

                results.append(mode_state)
                results[mode_num].filter_state(em_plugs)
                mode_title = test_mode.title()

                (comparison,error_count,_,total_compared) = reference_state.compare(results[mode_num], output_mode=comparison_mode, whitelist=per_context_nodes)

                TODO( 'Finish', 'Pass on the total_compared count in the results - be careful not to mess up QATA or the toolkit tests, and display the number in the toolkit results', 'MAYA-88339' )
                print 'A total of {} items were compared'.format( total_compared )

                if verbose:
                    comparisons[mode_title] = comparison
                else:
                    comparisons[mode_title] = error_count

                if mode_compare_files[mode_num] is not None:
                    with open(mode_compare_files[mode_num], 'w') as compare_file:
                        compare_file.write( str(comparison) )

        # Force restoration of EM state by leaving scope
    except Exception, ex:
        print 'WARNING: Test bailed out with "{}"'.format(ex)
    finally:
        cmds.cycleCheck( evaluation=old_cycle_check )

    return comparisons

# ===========================================================================
# Copyright 2018 Autodesk, Inc. All rights reserved.
#
# Use of this software is subject to the terms of the Autodesk license
# agreement provided at the time of installation or download, or which
# otherwise accompanies this software in either electronic or hard copy form.
# ===========================================================================
